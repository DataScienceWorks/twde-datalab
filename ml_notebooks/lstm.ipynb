{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LSTM with \n",
    "average features in terms of sales and promotion and weekly average features on them.\n",
    "\"\"\"\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For reading data from AWS S3\n",
    "\"\"\"\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import gzip\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = 'twde-datalab'\n",
    "train_key = 'raw/train.csv'\n",
    "test_key = 'raw/test.csv'\n",
    "items_key = 'raw/items.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_testdf(key):\n",
    "    obj = s3.Object(bucket,key)\n",
    "    data = obj.get()['Body'].read()\n",
    "    df = pd.read_csv(io.BytesIO(data), \n",
    "                    usecols=[0, 1, 2, 3, 4],\n",
    "                    dtype={'onpromotion': bool},\n",
    "                    parse_dates=[\"date\"] \n",
    "                    ).set_index(\n",
    "                        ['store_nbr', 'item_nbr', 'date'])\n",
    "    print(df.shape)\n",
    "    return df\n",
    "def get_traindf(key):\n",
    "    obj = s3.Object(bucket,key)\n",
    "    data = obj.get()['Body'].read()\n",
    "    traindf = pd.read_csv(io.BytesIO(data), \n",
    "                          usecols=[1, 2, 3, 4, 5],\n",
    "                          dtype={'onpromotion': bool},\n",
    "                          converters={'unit_sales': lambda u: np.log1p(\n",
    "                            float(u)) if float(u) > 0 else 0},\n",
    "                          parse_dates=[\"date\"],\n",
    "                          skiprows=range(1, 66458909)  # 2016-01-01\n",
    "                          )\n",
    "    return traindf\n",
    "\n",
    "def get_itemsdf(key):\n",
    "    obj = s3.Object(bucket,key)\n",
    "    data = obj.get()['Body'].read()\n",
    "    itemsdf = pd.read_csv(io.BytesIO(data)).set_index(\"item_nbr\")\n",
    "    return itemsdf\n",
    "\n",
    "def save_s3(df, key):\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "    csv_buffer.seek(0)\n",
    "    gz_buffer = io.BytesIO()\n",
    "\n",
    "    with gzip.GzipFile(mode='w', fileobj=gz_buffer) as gz_file:\n",
    "        gz_file.write(bytes(csv_buffer.getvalue(), 'utf-8'))\n",
    "\n",
    "    s3_object = s3.Object(bucket, key)\n",
    "    s3_object.put(Body=gz_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3370464, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train = get_traindf(train_key)\n",
    "df_test = get_testdf(test_key)\n",
    "items = get_itemsdf(items_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "==================================================\n",
      "Step 1\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.3982 - mean_squared_error: 0.3752 - val_loss: 0.3063 - val_mean_squared_error: 0.3063\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.3523 - mean_squared_error: 0.3334 - val_loss: 0.3011 - val_mean_squared_error: 0.3011\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.3455 - mean_squared_error: 0.3273 - val_loss: 0.2990 - val_mean_squared_error: 0.2990\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.3414 - mean_squared_error: 0.3235 - val_loss: 0.2989 - val_mean_squared_error: 0.2989\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.3392 - mean_squared_error: 0.3216 - val_loss: 0.2982 - val_mean_squared_error: 0.2982\n",
      "==================================================\n",
      "Step 2\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.3665 - mean_squared_error: 0.3457 - val_loss: 0.3305 - val_mean_squared_error: 0.3305\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.3627 - mean_squared_error: 0.3422 - val_loss: 0.3271 - val_mean_squared_error: 0.3271\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.3614 - mean_squared_error: 0.3411 - val_loss: 0.3276 - val_mean_squared_error: 0.3276\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.3606 - mean_squared_error: 0.3403 - val_loss: 0.3284 - val_mean_squared_error: 0.3284\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.3599 - mean_squared_error: 0.3397 - val_loss: 0.3271 - val_mean_squared_error: 0.3271\n",
      "==================================================\n",
      "Step 3\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.3711 - mean_squared_error: 0.3512 - val_loss: 0.3419 - val_mean_squared_error: 0.3419\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.3649 - mean_squared_error: 0.3456 - val_loss: 0.3407 - val_mean_squared_error: 0.3407\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.3638 - mean_squared_error: 0.3446 - val_loss: 0.3401 - val_mean_squared_error: 0.3401\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.3630 - mean_squared_error: 0.3439 - val_loss: 0.3410 - val_mean_squared_error: 0.3410\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.3626 - mean_squared_error: 0.3436 - val_loss: 0.3393 - val_mean_squared_error: 0.3393\n",
      "==================================================\n",
      "Step 4\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.3989 - mean_squared_error: 0.3770 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.3917 - mean_squared_error: 0.3704 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.3900 - mean_squared_error: 0.3688 - val_loss: 0.3562 - val_mean_squared_error: 0.3562\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.3893 - mean_squared_error: 0.3682 - val_loss: 0.3553 - val_mean_squared_error: 0.3553\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.3891 - mean_squared_error: 0.3681 - val_loss: 0.3541 - val_mean_squared_error: 0.3541\n",
      "==================================================\n",
      "Step 5\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.4100 - mean_squared_error: 0.3878 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.4021 - mean_squared_error: 0.3804 - val_loss: 0.3567 - val_mean_squared_error: 0.3567\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.4007 - mean_squared_error: 0.3791 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.4002 - mean_squared_error: 0.3787 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.3993 - mean_squared_error: 0.3779 - val_loss: 0.3590 - val_mean_squared_error: 0.3590\n",
      "==================================================\n",
      "Step 6\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.4002 - mean_squared_error: 0.3773 - val_loss: 0.3681 - val_mean_squared_error: 0.3681\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.3928 - mean_squared_error: 0.3704 - val_loss: 0.3638 - val_mean_squared_error: 0.3638\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.3912 - mean_squared_error: 0.3689 - val_loss: 0.3649 - val_mean_squared_error: 0.3649\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.3907 - mean_squared_error: 0.3685 - val_loss: 0.3686 - val_mean_squared_error: 0.3686\n",
      "Epoch 5/5\n",
      " - 10s - loss: 0.3904 - mean_squared_error: 0.3682 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "==================================================\n",
      "Step 7\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.3867 - mean_squared_error: 0.3654 - val_loss: 0.4382 - val_mean_squared_error: 0.4382\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.3812 - mean_squared_error: 0.3604 - val_loss: 0.4078 - val_mean_squared_error: 0.4078\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.3800 - mean_squared_error: 0.3593 - val_loss: 0.4239 - val_mean_squared_error: 0.4239\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.3792 - mean_squared_error: 0.3586 - val_loss: 0.4331 - val_mean_squared_error: 0.4331\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.3791 - mean_squared_error: 0.3585 - val_loss: 0.4186 - val_mean_squared_error: 0.4186\n",
      "==================================================\n",
      "Step 8\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.3750 - mean_squared_error: 0.3558 - val_loss: 0.3961 - val_mean_squared_error: 0.3961\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.3676 - mean_squared_error: 0.3492 - val_loss: 0.3864 - val_mean_squared_error: 0.3864\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.3662 - mean_squared_error: 0.3479 - val_loss: 0.3910 - val_mean_squared_error: 0.3910\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.3654 - mean_squared_error: 0.3472 - val_loss: 0.4095 - val_mean_squared_error: 0.4095\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.3649 - mean_squared_error: 0.3468 - val_loss: 0.4079 - val_mean_squared_error: 0.4079\n",
      "==================================================\n",
      "Step 9\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.3811 - mean_squared_error: 0.3600 - val_loss: 0.3791 - val_mean_squared_error: 0.3791\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.3753 - mean_squared_error: 0.3546 - val_loss: 0.3728 - val_mean_squared_error: 0.3728\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.3743 - mean_squared_error: 0.3536 - val_loss: 0.3830 - val_mean_squared_error: 0.3830\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.3738 - mean_squared_error: 0.3532 - val_loss: 0.3807 - val_mean_squared_error: 0.3807\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.3734 - mean_squared_error: 0.3529 - val_loss: 0.3867 - val_mean_squared_error: 0.3867\n",
      "==================================================\n",
      "Step 10\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.3883 - mean_squared_error: 0.3679 - val_loss: 0.3768 - val_mean_squared_error: 0.3768\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.3804 - mean_squared_error: 0.3607 - val_loss: 0.3732 - val_mean_squared_error: 0.3732\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.3793 - mean_squared_error: 0.3598 - val_loss: 0.3725 - val_mean_squared_error: 0.3725\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.3784 - mean_squared_error: 0.3590 - val_loss: 0.3726 - val_mean_squared_error: 0.3726\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.3779 - mean_squared_error: 0.3585 - val_loss: 0.3662 - val_mean_squared_error: 0.3662\n",
      "==================================================\n",
      "Step 11\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.4210 - mean_squared_error: 0.3984 - val_loss: 0.3780 - val_mean_squared_error: 0.3780\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.4120 - mean_squared_error: 0.3902 - val_loss: 0.3790 - val_mean_squared_error: 0.3790\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.4106 - mean_squared_error: 0.3889 - val_loss: 0.3768 - val_mean_squared_error: 0.3768\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.4094 - mean_squared_error: 0.3878 - val_loss: 0.3764 - val_mean_squared_error: 0.3764\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.4092 - mean_squared_error: 0.3875 - val_loss: 0.3753 - val_mean_squared_error: 0.3753\n",
      "==================================================\n",
      "Step 12\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.4324 - mean_squared_error: 0.4096 - val_loss: 0.3900 - val_mean_squared_error: 0.3900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      " - 9s - loss: 0.4233 - mean_squared_error: 0.4011 - val_loss: 0.3883 - val_mean_squared_error: 0.3883\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.4219 - mean_squared_error: 0.3998 - val_loss: 0.3879 - val_mean_squared_error: 0.3879\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.4211 - mean_squared_error: 0.3991 - val_loss: 0.3889 - val_mean_squared_error: 0.3889\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.4206 - mean_squared_error: 0.3986 - val_loss: 0.3880 - val_mean_squared_error: 0.3880\n",
      "==================================================\n",
      "Step 13\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.4141 - mean_squared_error: 0.3909 - val_loss: 0.3784 - val_mean_squared_error: 0.3784\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.4065 - mean_squared_error: 0.3838 - val_loss: 0.3860 - val_mean_squared_error: 0.3860\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.4052 - mean_squared_error: 0.3825 - val_loss: 0.3793 - val_mean_squared_error: 0.3793\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.4043 - mean_squared_error: 0.3818 - val_loss: 0.3770 - val_mean_squared_error: 0.3770\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.4038 - mean_squared_error: 0.3813 - val_loss: 0.3792 - val_mean_squared_error: 0.3792\n",
      "==================================================\n",
      "Step 14\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.3993 - mean_squared_error: 0.3775 - val_loss: 0.3650 - val_mean_squared_error: 0.3650\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.3939 - mean_squared_error: 0.3727 - val_loss: 0.3652 - val_mean_squared_error: 0.3652\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.3929 - mean_squared_error: 0.3717 - val_loss: 0.3647 - val_mean_squared_error: 0.3647\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.3925 - mean_squared_error: 0.3714 - val_loss: 0.3641 - val_mean_squared_error: 0.3641\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.3919 - mean_squared_error: 0.3709 - val_loss: 0.3638 - val_mean_squared_error: 0.3638\n",
      "==================================================\n",
      "Step 15\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.3888 - mean_squared_error: 0.3691 - val_loss: 0.3528 - val_mean_squared_error: 0.3528\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.3808 - mean_squared_error: 0.3620 - val_loss: 0.3516 - val_mean_squared_error: 0.3516\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.3801 - mean_squared_error: 0.3613 - val_loss: 0.3522 - val_mean_squared_error: 0.3522\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.3790 - mean_squared_error: 0.3603 - val_loss: 0.3555 - val_mean_squared_error: 0.3555\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.3785 - mean_squared_error: 0.3599 - val_loss: 0.3513 - val_mean_squared_error: 0.3513\n",
      "==================================================\n",
      "Step 16\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      " - 9s - loss: 0.3974 - mean_squared_error: 0.3755 - val_loss: 0.3759 - val_mean_squared_error: 0.3759\n",
      "Epoch 2/5\n",
      " - 9s - loss: 0.3910 - mean_squared_error: 0.3695 - val_loss: 0.3755 - val_mean_squared_error: 0.3755\n",
      "Epoch 3/5\n",
      " - 9s - loss: 0.3898 - mean_squared_error: 0.3684 - val_loss: 0.3728 - val_mean_squared_error: 0.3728\n",
      "Epoch 4/5\n",
      " - 9s - loss: 0.3888 - mean_squared_error: 0.3675 - val_loss: 0.3719 - val_mean_squared_error: 0.3719\n",
      "Epoch 5/5\n",
      " - 9s - loss: 0.3887 - mean_squared_error: 0.3674 - val_loss: 0.3716 - val_mean_squared_error: 0.3716\n",
      "Unweighted validation mse:  0.365463224588\n",
      "Full validation mse:        0.364897677619\n",
      "'Public' validation mse:    0.335706145958\n",
      "'Private' validation mse:   0.378166555647\n"
     ]
    }
   ],
   "source": [
    "df_2017 = df_train.loc[df_train.date>=pd.datetime(2017,1,1)]\n",
    "del df_train\n",
    "\n",
    "promo_2017_train = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(\n",
    "        level=-1).fillna(False)\n",
    "promo_2017_train.columns = promo_2017_train.columns.get_level_values(1)\n",
    "promo_2017_test = df_test[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_2017_test.columns = promo_2017_test.columns.get_level_values(1)\n",
    "promo_2017_test = promo_2017_test.reindex(promo_2017_train.index).fillna(False)\n",
    "promo_2017 = pd.concat([promo_2017_train, promo_2017_test], axis=1)\n",
    "del promo_2017_test, promo_2017_train\n",
    "\n",
    "df_2017 = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(\n",
    "        level=-1).fillna(0)\n",
    "df_2017.columns = df_2017.columns.get_level_values(1)\n",
    "\n",
    "items = items.reindex(df_2017.index.get_level_values(1))\n",
    "\n",
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]\n",
    "\n",
    "def prepare_dataset(t2017, is_train=True):\n",
    "    X = pd.DataFrame({\n",
    "        \"day_1_2017\": get_timespan(df_2017, t2017, 1, 1).values.ravel(),\n",
    "        \"mean_3_2017\": get_timespan(df_2017, t2017, 3, 3).mean(axis=1).values,\n",
    "        \"mean_7_2017\": get_timespan(df_2017, t2017, 7, 7).mean(axis=1).values,\n",
    "        \"mean_14_2017\": get_timespan(df_2017, t2017, 14, 14).mean(axis=1).values,\n",
    "        \"mean_30_2017\": get_timespan(df_2017, t2017, 30, 30).mean(axis=1).values,\n",
    "        \"mean_60_2017\": get_timespan(df_2017, t2017, 60, 60).mean(axis=1).values,\n",
    "        \"mean_140_2017\": get_timespan(df_2017, t2017, 140, 140).mean(axis=1).values,\n",
    "        \"promo_14_2017\": get_timespan(promo_2017, t2017, 14, 14).sum(axis=1).values,\n",
    "        \"promo_60_2017\": get_timespan(promo_2017, t2017, 60, 60).sum(axis=1).values,\n",
    "        \"promo_140_2017\": get_timespan(promo_2017, t2017, 140, 140).sum(axis=1).values\n",
    "    })\n",
    "    for i in range(7):\n",
    "        X['mean_4_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "        X['mean_20_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 140-i, 20, freq='7D').mean(axis=1).values\n",
    "    for i in range(16):\n",
    "        X[\"promo_{}\".format(i)] = promo_2017[\n",
    "            t2017 + timedelta(days=i)].values.astype(np.uint8)\n",
    "    if is_train:\n",
    "        y = df_2017[\n",
    "            pd.date_range(t2017, periods=16)\n",
    "        ].values\n",
    "        return X, y\n",
    "    return X\n",
    "\n",
    "print(\"Preparing dataset\")\n",
    "t2017 = date(2017, 5, 31)\n",
    "X_l, y_l = [], []\n",
    "for i in range(6):\n",
    "    delta = timedelta(days=7 * i)\n",
    "    X_tmp, y_tmp = prepare_dataset(\n",
    "        t2017 + delta\n",
    "    )\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "del X_l, y_l\n",
    "X_val, y_val = prepare_dataset(date(2017, 7, 26))\n",
    "X_test = prepare_dataset(date(2017, 8, 16), is_train=False)\n",
    "\n",
    "stores_items = pd.DataFrame(index=df_2017.index)\n",
    "test_ids = df_test[['id']]\n",
    "\n",
    "items = items.reindex( stores_items.index.get_level_values(1) )\n",
    "\n",
    "X_train = X_train.as_matrix()\n",
    "X_test = X_test.as_matrix()\n",
    "X_val = X_val.as_matrix()\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(32))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss = 'mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "sample_weights=np.array( pd.concat([items[\"perishable\"]] * 6) * 0.25 + 1 )\n",
    "for i in range(16):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Step %d\" % (i+1))\n",
    "    print(\"=\" * 50)\n",
    "    y = y_train[:, i]\n",
    "    xv = X_val\n",
    "    yv = y_val[:, i]\n",
    "    model.fit(X_train, y, batch_size = 512, epochs = N_EPOCHS, verbose=2,\n",
    "               sample_weight=sample_weights, validation_data=(xv,yv) ) \n",
    "    val_pred.append(model.predict(X_val))\n",
    "    test_pred.append(model.predict(X_test))\n",
    "    \n",
    "n_public = 5 # Number of days in public test set\n",
    "weights=pd.concat([items[\"perishable\"]]) * 0.25 + 1\n",
    "print(\"Unweighted validation mse: \", mean_squared_error(\n",
    "    y_val, np.array(val_pred).squeeze(axis=2).transpose()) )\n",
    "print(\"Full validation mse:       \", mean_squared_error(\n",
    "    y_val, np.array(val_pred).squeeze(axis=2).transpose(), sample_weight=weights) )\n",
    "print(\"'Public' validation mse:   \", mean_squared_error(\n",
    "    y_val[:,:n_public], np.array(val_pred).squeeze(axis=2).transpose()[:,:n_public], \n",
    "    sample_weight=weights) )\n",
    "print(\"'Private' validation mse:  \", mean_squared_error(\n",
    "    y_val[:,n_public:], np.array(val_pred).squeeze(axis=2).transpose()[:,n_public:], \n",
    "    sample_weight=weights) )\n",
    "    \n",
    "y_test = np.array(test_pred).squeeze(axis=2).transpose()\n",
    "df_preds = pd.DataFrame(\n",
    "    y_test, index=stores_items.index,\n",
    "    columns=pd.date_range(\"2017-08-16\", periods=16)\n",
    ").stack().to_frame(\"unit_sales\")\n",
    "df_preds.index.set_names([\"store_nbr\", \"item_nbr\", \"date\"], inplace=True)\n",
    "\n",
    "submission = test_ids.join(df_preds, how=\"left\").fillna(0)\n",
    "submission[\"unit_sales\"] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_s3(submission, 'submission/lstm.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
